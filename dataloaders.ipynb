{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "id": "uDb9BLWu4uW-"
     },
     "source": [
      "# Profiling memory loading\n",
      "Another crucial part of training a deep learning model is the dataset, and of course loading the dataset from disk. This task can be parallelized, and we will now investigate how the number of CPU loaders affect the time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import torch\n",
      "from torch import nn\n",
      "from torch.profiler import profile, record_function, ProfilerActivity\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms"
     ],
     "language": "python",
     "metadata": {
      "id": "W73hMEoHNw_s"
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = \"./data\"\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "batch_size = 64"
     ],
     "language": "python",
     "metadata": {
      "id": "itpSTtyAeR8B"
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "transform_train = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR100(\n",
      "    root=path, train=True, download=True, transform=transform_train)"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "yxzlIGeEePat",
      "outputId": "b2545e31-9380-41e9-ff3a-f574625694d8"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Files already downloaded and verified\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data(dataset, num_workers, sort_string):\n",
      "    trainloader = torch.utils.data.DataLoader(\n",
      "        dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
      "    \n",
      "    trainloader_enum = enumerate(trainloader)\n",
      "\n",
      "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
      "        with record_function(\"data_load\"):\n",
      "            batch_idx, (inputs, targets) = next(trainloader_enum)\n",
      "            inputs, targets = inputs.to(device), targets.to(device)       \n",
      "\n",
      "    print(prof.key_averages().table(sort_by=sort_string, row_limit=15))"
     ],
     "language": "python",
     "metadata": {
      "id": "kk387JXbeNug"
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "id": "yxwpniRs5qGI"
     },
     "source": [
      "1 Loader"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "workers = 1\n",
      "sort_string = \"cuda_time_total\"\n",
      "load_data(trainset, workers, sort_string)"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "1LPzBLow4n4_",
      "outputId": "8e3ebf5e-bea4-4979-f947-9c1a7d4e69f4"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                            aten::zeros         2.10%       1.445ms         2.13%       1.460ms     730.000us             2  \n",
        "                                            aten::empty         0.04%      28.000us         0.04%      28.000us       4.667us             6  \n",
        "                                            aten::zero_         0.01%       4.000us         0.01%       4.000us       2.000us             2  \n",
        "                                              data_load         3.05%       2.095ms        97.90%      67.254ms      67.254ms             1  \n",
        "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        94.72%      65.069ms        94.81%      65.136ms      65.136ms             1  \n",
        "                                               aten::to         0.01%       7.000us         0.01%       7.000us       1.750us             4  \n",
        "                                          aten::detach_         0.01%      10.000us         0.02%      17.000us       8.500us             2  \n",
        "                                                detach_         0.01%       7.000us         0.01%       7.000us       3.500us             2  \n",
        "                                             aten::set_         0.05%      34.000us         0.05%      34.000us      17.000us             2  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "Self CPU time total: 68.699ms\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/opt/conda/lib/python3.7/site-packages/torch/autograd/profiler.py:151: UserWarning: CUDA is not available, disabling CUDA profiling\n",
        "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "id": "Rpx95fIMAzut"
     },
     "source": [
      "2 loaders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "workers = 2\n",
      "sort_string = \"self_cuda_time_total\"\n",
      "load_data(trainset, workers, sort_string)"
     ],
     "language": "python",
     "metadata": {
      "id": "ENAVnl-VAzuu",
      "outputId": "fad112b4-0ad5-4419-ad0d-4f2b9bf95f77"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                            aten::zeros         0.22%      83.000us         0.30%     112.000us      56.000us             2  \n",
        "                                            aten::empty         0.12%      47.000us         0.12%      47.000us       7.833us             6  \n",
        "                                            aten::zero_         0.01%       4.000us         0.01%       4.000us       2.000us             2  \n",
        "                                              data_load         0.52%     197.000us        99.75%      37.785ms      37.785ms             1  \n",
        "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        98.99%      37.498ms        99.16%      37.563ms      37.563ms             1  \n",
        "                                               aten::to         0.02%       9.000us         0.02%       9.000us       2.250us             4  \n",
        "                                          aten::detach_         0.03%      13.000us         0.07%      26.000us      13.000us             2  \n",
        "                                                detach_         0.03%      13.000us         0.03%      13.000us       6.500us             2  \n",
        "                                             aten::set_         0.04%      16.000us         0.04%      16.000us       8.000us             2  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "Self CPU time total: 37.880ms\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "id": "SDmijYcAAzuu"
     },
     "source": [
      "4 loaders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "workers = 4\n",
      "sort_string = \"self_cuda_time_total\"\n",
      "load_data(trainset, workers, sort_string)"
     ],
     "language": "python",
     "metadata": {
      "id": "_c9EE4TPAzuv",
      "outputId": "47329068-e028-4b05-e1ca-3df88c907cb8"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                            aten::zeros         0.26%      65.000us         0.31%      79.000us      39.500us             2  \n",
        "                                            aten::empty         0.15%      38.000us         0.15%      38.000us       6.333us             6  \n",
        "                                            aten::zero_         0.02%       4.000us         0.02%       4.000us       2.000us             2  \n",
        "                                              data_load         0.77%     195.000us        99.75%      25.240ms      25.240ms             1  \n",
        "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        98.66%      24.962ms        98.89%      25.022ms      25.022ms             1  \n",
        "                                               aten::to         0.03%       7.000us         0.03%       7.000us       1.750us             4  \n",
        "                                          aten::detach_         0.04%       9.000us         0.07%      18.000us       9.000us             2  \n",
        "                                                detach_         0.04%       9.000us         0.04%       9.000us       4.500us             2  \n",
        "                                             aten::set_         0.05%      13.000us         0.05%      13.000us       6.500us             2  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "Self CPU time total: 25.302ms\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "id": "fjC4BrAuAzuw"
     },
     "source": [
      "8 loaders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "workers = 8\n",
      "sort_string = \"cuda_time_total\"\n",
      "load_data(trainset, workers, sort_string)"
     ],
     "language": "python",
     "metadata": {
      "id": "pu6Lo8HsAzuw",
      "outputId": "9a71bff5-987e-4c42-feaa-091d72f39baa"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                            aten::zeros         0.34%     100.000us         0.42%     123.000us      61.500us             2  \n",
        "                                            aten::empty         0.20%      60.000us         0.20%      60.000us      10.000us             6  \n",
        "                                            aten::zero_         0.02%       5.000us         0.02%       5.000us       2.500us             2  \n",
        "                                              data_load         0.69%     205.000us        99.70%      29.481ms      29.481ms             1  \n",
        "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        98.55%      29.143ms        98.85%      29.232ms      29.232ms             1  \n",
        "                                               aten::to         0.04%      12.000us         0.04%      12.000us       3.000us             4  \n",
        "                                          aten::detach_         0.05%      15.000us         0.10%      30.000us      15.000us             2  \n",
        "                                                detach_         0.05%      15.000us         0.05%      15.000us       7.500us             2  \n",
        "                                             aten::set_         0.05%      16.000us         0.05%      16.000us       8.000us             2  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "Self CPU time total: 29.571ms\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "id": "1gxVnQ_hAzuy"
     },
     "source": [
      "16 loaders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "workers = 16\n",
      "sort_string = \"cuda_time_total\"\n",
      "load_data(trainset, workers, sort_string)"
     ],
     "language": "python",
     "metadata": {
      "id": "ExhAdY9MAzuy",
      "outputId": "ca0c474f-ff8f-433d-f339-246cf485e20a"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
        "  cpuset_checked))\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "                                            aten::zeros        28.93%       6.373ms        29.00%       6.388ms       3.194ms             2  \n",
        "                                            aten::empty         0.12%      26.000us         0.12%      26.000us       4.333us             6  \n",
        "                                            aten::zero_         0.02%       5.000us         0.02%       5.000us       2.500us             2  \n",
        "                                              data_load         0.77%     170.000us        71.10%      15.663ms      15.663ms             1  \n",
        "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        69.84%      15.386ms        70.20%      15.465ms      15.465ms             1  \n",
        "                                               aten::to         0.04%       9.000us         0.04%       9.000us       2.250us             4  \n",
        "                                          aten::detach_         0.17%      37.000us         0.22%      49.000us      24.500us             2  \n",
        "                                                detach_         0.05%      12.000us         0.05%      12.000us       6.000us             2  \n",
        "                                             aten::set_         0.06%      13.000us         0.06%      13.000us       6.500us             2  \n",
        "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
        "Self CPU time total: 22.031ms\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "id": "yXBQS8vTA5iB"
     },
     "source": [
      "# Analysis\n",
      "\n",
      "Parallelizing data loading has the potential to give massive speedups. As can be seen above, the running time decreases from over 5s with one dataloader to dozens of milliseconds with 2 loaders. The speedup continues with more dataloaders, until 8 loaders. When using 16 loaders, we instead see an increase in loading time. The system outputs a warning that this is above the recommended maximum, and this is where the communication overhead of the 16 processes becomes larger than the efficiency gain. Overall, we achieve a speedup greater than 300 for using 8 loaders over 1, although this is likely a fluke result as the system may need to warmup. Compared to 2 loaders, 8 loaders achieved a 2.6 time speedup, which is worse than linear but an important observation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "q2KwbT_XCM3t"
     },
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}
