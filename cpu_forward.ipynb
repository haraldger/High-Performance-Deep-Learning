{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
        "from resnet import resnet18_bottleneck, resnet34_bottleneck, resnet50_bottleneck, resnet101_bottleneck, resnet152_bottleneck\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "v6p8WDdTs-Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiling resnets - Forward\n",
        "\n",
        "Initially, we profile the forward passes only of various resnets."
      ],
      "metadata": {
        "id": "w3Okhl1RgLNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "def profile_model(model, inputs, sort_by_self=True):\n",
        "    sort_by_string = \"self_cpu_time_total\" if sort_by_self else \"cpu_time_total\"\n",
        "    model(inputs)   # Warmup\n",
        "\n",
        "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "        with record_function(\"model_inference\"):\n",
        "            model(inputs)\n",
        "\n",
        "    print(prof.key_averages().table(sort_by=sort_by_string, row_limit=15))"
      ],
      "metadata": {
        "id": "EBqrdvFrgJgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-18"
      ],
      "metadata": {
        "id": "wz2qkHh3jISH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWCPD-tngPgQ",
        "outputId": "3011746a-b03a-45a1-bf21-3bb34e956db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        74.35%        1.955s        74.46%        1.958s      97.915ms            20  \n",
            "          aten::native_batch_norm        11.88%     312.413ms        11.96%     314.581ms      15.729ms            20  \n",
            "    aten::max_pool2d_with_indices        10.93%     287.447ms        10.93%     287.447ms     287.447ms             1  \n",
            "                  aten::clamp_min         1.44%      37.938ms         1.44%      37.938ms       2.371ms            16  \n",
            "                       aten::add_         0.69%      18.194ms         0.69%      18.194ms     649.786us            28  \n",
            "                  model_inference         0.33%       8.687ms       100.00%        2.630s        2.630s             1  \n",
            "                      aten::empty         0.19%       4.960ms         0.19%       4.960ms      24.800us           200  \n",
            "               aten::_convolution         0.03%     912.000us        74.50%        1.959s      97.961ms            20  \n",
            "                      aten::addmm         0.03%     834.000us         0.03%     870.000us     870.000us             1  \n",
            "                       aten::relu         0.03%     790.000us         1.47%      38.728ms       2.421ms            16  \n",
            "                aten::convolution         0.03%     726.000us        74.52%        1.960s      97.997ms            20  \n",
            "                        aten::sum         0.01%     370.000us         0.01%     383.000us     383.000us             1  \n",
            "     aten::_batch_norm_impl_index         0.01%     312.000us        11.98%     314.970ms      15.748ms            20  \n",
            "                     aten::conv2d         0.01%     305.000us        74.53%        1.960s      98.012ms            20  \n",
            "                aten::as_strided_         0.01%     197.000us         0.01%     197.000us       9.850us            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.630s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-34"
      ],
      "metadata": {
        "id": "CynuLFec0MA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet34()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IwY2pwdqb3k",
        "outputId": "7d60ace4-ce35-4c19-913f-f9fb635a806e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        83.78%        4.992s        83.85%        4.996s     138.788ms            36  \n",
            "          aten::native_batch_norm         9.00%     536.570ms         9.06%     540.098ms      15.003ms            36  \n",
            "    aten::max_pool2d_with_indices         4.90%     292.017ms         4.90%     292.017ms     292.017ms             1  \n",
            "                  aten::clamp_min         1.18%      70.235ms         1.18%      70.235ms       2.195ms            32  \n",
            "                       aten::add_         0.61%      36.113ms         0.61%      36.113ms     694.481us            52  \n",
            "                  model_inference         0.27%      16.374ms       100.00%        5.959s        5.959s             1  \n",
            "                      aten::empty         0.12%       7.423ms         0.12%       7.423ms      20.619us           360  \n",
            "                       aten::relu         0.03%       1.906ms         1.21%      72.141ms       2.254ms            32  \n",
            "                aten::convolution         0.02%       1.401ms        83.89%        4.999s     138.851ms            36  \n",
            "                      aten::addmm         0.02%     931.000us         0.02%     963.000us     963.000us             1  \n",
            "               aten::_convolution         0.01%     855.000us        83.87%        4.997s     138.812ms            36  \n",
            "     aten::_batch_norm_impl_index         0.01%     695.000us         9.08%     541.069ms      15.030ms            36  \n",
            "                     aten::conv2d         0.01%     436.000us        83.90%        4.999s     138.863ms            36  \n",
            "                aten::as_strided_         0.01%     377.000us         0.01%     377.000us      10.472us            36  \n",
            "                        aten::sum         0.01%     370.000us         0.01%     382.000us     382.000us             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.959s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-50"
      ],
      "metadata": {
        "id": "T9t3PtYw0bh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb2kygQ10bCt",
        "outputId": "e97b7842-1b64-4c61-a035-bc15d6cffbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        85.04%        5.300s        85.10%        5.304s     147.335ms            36  \n",
            "          aten::native_batch_norm         8.11%     505.444ms         8.17%     509.183ms      14.144ms            36  \n",
            "    aten::max_pool2d_with_indices         4.66%     290.535ms         4.66%     290.535ms     290.535ms             1  \n",
            "                  aten::clamp_min         1.12%      69.988ms         1.12%      69.988ms       2.187ms            32  \n",
            "                       aten::add_         0.56%      34.603ms         0.56%      34.603ms     665.442us            52  \n",
            "                  model_inference         0.26%      16.029ms       100.00%        6.232s        6.232s             1  \n",
            "                      aten::empty         0.13%       7.869ms         0.13%       7.869ms      21.858us           360  \n",
            "                aten::convolution         0.03%       1.887ms        85.15%        5.307s     147.411ms            36  \n",
            "                       aten::relu         0.03%       1.864ms         1.15%      71.852ms       2.245ms            32  \n",
            "                      aten::addmm         0.01%     892.000us         0.01%     924.000us     924.000us             1  \n",
            "               aten::_convolution         0.01%     856.000us        85.12%        5.305s     147.359ms            36  \n",
            "     aten::_batch_norm_impl_index         0.01%     668.000us         8.19%     510.245ms      14.173ms            36  \n",
            "                        aten::sum         0.01%     430.000us         0.01%     442.000us     442.000us             1  \n",
            "                     aten::conv2d         0.01%     414.000us        85.16%        5.307s     147.423ms            36  \n",
            "                aten::as_strided_         0.01%     394.000us         0.01%     394.000us      10.944us            36  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 6.232s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-101"
      ],
      "metadata": {
        "id": "VBTHUVpr0pBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet101()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz1Ts7B00lw6",
        "outputId": "8a774df9-9abf-4fac-c1d1-60ca0c9354cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        86.31%        8.672s        86.38%        8.679s     123.993ms            70  \n",
            "          aten::native_batch_norm         7.71%     774.586ms         7.77%     781.175ms      11.160ms            70  \n",
            "    aten::max_pool2d_with_indices         3.74%     375.532ms         3.74%     375.532ms     375.532ms             1  \n",
            "                  aten::clamp_min         1.12%     112.486ms         1.12%     112.486ms       1.704ms            66  \n",
            "                       aten::add_         0.56%      56.467ms         0.56%      56.467ms     548.223us           103  \n",
            "                  model_inference         0.30%      30.249ms       100.00%       10.048s       10.048s             1  \n",
            "                      aten::empty         0.14%      13.846ms         0.14%      13.846ms      19.780us           700  \n",
            "                       aten::relu         0.03%       3.183ms         1.15%     115.669ms       1.753ms            66  \n",
            "                aten::convolution         0.03%       2.624ms        86.42%        8.684s     124.054ms            70  \n",
            "               aten::_convolution         0.02%       1.657ms        86.39%        8.681s     124.016ms            70  \n",
            "     aten::_batch_norm_impl_index         0.01%       1.099ms         7.80%     783.543ms      11.193ms            70  \n",
            "                     aten::conv2d         0.01%     919.000us        86.43%        8.685s     124.067ms            70  \n",
            "                      aten::addmm         0.01%     860.000us         0.01%     898.000us     898.000us             1  \n",
            "                aten::as_strided_         0.01%     711.000us         0.01%     711.000us      10.157us            70  \n",
            "                 aten::batch_norm         0.00%     464.000us         7.80%     784.007ms      11.200ms            70  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 10.048s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-152"
      ],
      "metadata": {
        "id": "UV3LHe-j3knq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet152()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tddk5pda0rru",
        "outputId": "ddd67082-f7cc-4c0e-8b83-14e91e2ce477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        88.81%       13.168s        88.88%       13.178s     126.716ms           104  \n",
            "          aten::native_batch_norm         6.98%        1.034s         7.05%        1.045s      10.046ms           104  \n",
            "    aten::max_pool2d_with_indices         1.95%     289.665ms         1.95%     289.665ms     289.665ms             1  \n",
            "                  aten::clamp_min         1.14%     169.396ms         1.14%     169.396ms       1.694ms           100  \n",
            "                       aten::add_         0.56%      82.569ms         0.56%      82.569ms     536.162us           154  \n",
            "                  model_inference         0.29%      43.631ms       100.00%       14.826s       14.826s             1  \n",
            "                      aten::empty         0.14%      20.917ms         0.14%      20.917ms      20.113us          1040  \n",
            "                       aten::relu         0.03%       4.593ms         1.17%     173.989ms       1.740ms           100  \n",
            "                aten::convolution         0.03%       3.808ms        88.93%       13.185s     126.775ms           104  \n",
            "               aten::_convolution         0.02%       2.361ms        88.90%       13.181s     126.738ms           104  \n",
            "     aten::_batch_norm_impl_index         0.01%       1.693ms         7.07%        1.048s      10.073ms           104  \n",
            "                      aten::addmm         0.01%       1.357ms         0.01%       1.392ms       1.392ms             1  \n",
            "                     aten::conv2d         0.01%       1.174ms        88.93%       13.186s     126.786ms           104  \n",
            "                aten::as_strided_         0.01%       1.069ms         0.01%       1.069ms      10.279us           104  \n",
            "                 aten::batch_norm         0.00%     733.000us         7.07%        1.048s      10.080ms           104  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 14.826s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Standard ResNets on the CPU is fairly slow because there are several connections in each layer. Most of the time is taken in the convolution layers which forms sort of a performance \"bottleneck\" here."
      ],
      "metadata": {
        "id": "epHbiKalkt1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bottleneck ResNets\n",
        "\n",
        "### Bottleneck RN18"
      ],
      "metadata": {
        "id": "xaGw-ZterC4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18_bottleneck()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu8nco1c3nVV",
        "outputId": "a1d829bb-099b-41ff-b535-fc1c52a906ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        39.38%     445.910ms        39.55%     447.853ms      15.995ms            28  \n",
            "          aten::native_batch_norm        27.57%     312.195ms        27.69%     313.598ms      11.200ms            28  \n",
            "    aten::max_pool2d_with_indices        26.14%     296.015ms        26.14%     296.015ms     296.015ms             1  \n",
            "                  aten::clamp_min         2.47%      28.012ms         2.47%      28.012ms       1.167ms            24  \n",
            "                  model_inference         2.13%      24.066ms       100.00%        1.132s        1.132s             1  \n",
            "                       aten::add_         1.62%      18.321ms         1.62%      18.321ms     508.917us            36  \n",
            "                      aten::empty         0.28%       3.184ms         0.28%       3.184ms      11.371us           280  \n",
            "                      aten::addmm         0.07%     834.000us         0.08%     871.000us     871.000us             1  \n",
            "                aten::convolution         0.07%     794.000us        39.66%     449.182ms      16.042ms            28  \n",
            "                       aten::relu         0.06%     725.000us         2.54%      28.737ms       1.197ms            24  \n",
            "               aten::_convolution         0.05%     535.000us        39.59%     448.388ms      16.014ms            28  \n",
            "                        aten::sum         0.03%     382.000us         0.03%     394.000us     394.000us             1  \n",
            "     aten::_batch_norm_impl_index         0.03%     327.000us        27.74%     314.123ms      11.219ms            28  \n",
            "                     aten::conv2d         0.02%     257.000us        39.69%     449.439ms      16.051ms            28  \n",
            "                aten::as_strided_         0.02%     251.000us         0.02%     251.000us       8.964us            28  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.132s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck RN34"
      ],
      "metadata": {
        "id": "CMQ5pCdHr8E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet34_bottleneck()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70swjCIirfR7",
        "outputId": "32c4fb9f-af2b-4884-c806-8278e3b12ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         2.18%      32.382ms       100.00%        1.488s        1.488s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.488s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck RN50"
      ],
      "metadata": {
        "id": "saVj1uCGr3SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50_bottleneck()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9064BtjyrtM9",
        "outputId": "3e8488a2-d2c4-432e-eeda-5be1175659a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         2.06%      28.524ms       100.00%        1.386s        1.386s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.386s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck RN 101"
      ],
      "metadata": {
        "id": "DsPWX3FxsAjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet101_bottleneck()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1gjt7F8sD7D",
        "outputId": "16b1712f-46d9-4309-9ff5-6bea71c2b393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         2.01%      38.184ms       100.00%        1.898s        1.898s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.898s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck RN152"
      ],
      "metadata": {
        "id": "IlWV8M89sM6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet152_bottleneck()\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOLnrJxzsIV-",
        "outputId": "725ccb4d-4d94-4d0f-bd97-e70ef9da61e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         2.29%      60.595ms       100.00%        2.644s        2.644s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.644s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the Bottleneck ResNets run much faster already just running them on CPU. This is because the bottleneck layers have much less parameters and therefore they run significantly faster."
      ],
      "metadata": {
        "id": "yKJ9sLKotV44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large ResNets\n",
        "\n",
        "Huge gains are observed, even with large-mode resnets where the number of internal channels have been quadrupled:\n"
      ],
      "metadata": {
        "id": "yQHPrYPjsfNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large ResNet-50"
      ],
      "metadata": {
        "id": "V1Jr4K6Bux6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50_bottleneck(large_mode=True)\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsWNDoWpsS98",
        "outputId": "cfc8c279-0f6f-4f70-a601-7758eba5e5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         0.98%      28.007ms       100.00%        2.845s        2.845s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.845s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large ResNet-101 \n"
      ],
      "metadata": {
        "id": "fip3AaCWu2Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet101_bottleneck(large_mode=True)\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FMCsXUPssbc",
        "outputId": "8a00d662-204a-42b1-8af9-f73b0a4539f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         1.34%      51.297ms       100.00%        3.827s        3.827s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.827s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large ResNet-152"
      ],
      "metadata": {
        "id": "AWCUnuqVu9qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet152_bottleneck(large_mode=True)\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "profile_model(model, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-SHaE5ku8wr",
        "outputId": "17007c13-0a0a-481b-cafb-36db0a9ae3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "This report only display top-level ops statistics\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         1.27%      68.666ms       100.00%        5.389s        5.389s             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.389s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "Adding bottleneck layers drastically reduces computation time of the forward pass. Standard ResNets (without bottleneck layers) are about 5 times slower than the bottleneck counterparts. For excessively large ResNets, where the number of hidden channels is quadrupled, execution time is only approximately doubled and remains at roughly 1/3 the time of the corresponding plain ResNet.\n",
        "\n",
        "Using bottleneck layers brings down the proportion of time spent performing convolution drastically. For the plain nets, we observe that between 80% and 90% of execution time is spent on convolution (the MKL implementation aten::mkldnn_convolution). When employing bottlenecks, this is instead 40-45%. Even for the extra large ResNet152, the computation time spent on convolution is only 70%, and its absolute execution time is lower than that of the plain ResNet18 (3.72s vs 3.85s)."
      ],
      "metadata": {
        "id": "J_BoX9YyvMgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward pass and Backward pass\n"
      ],
      "metadata": {
        "id": "diPjmjon1X96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we investigate the execution time of a full forward and backward pass."
      ],
      "metadata": {
        "id": "Fe2OTkG623sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet152_bottleneck(large_mode=True)\n",
        "inputs = torch.rand(32, 3, 224, 224)    # Batch size 32\n",
        "targets = torch.randint(1000, size=(32,))\n",
        "\n",
        "model(inputs)  # Warmup\n",
        "\n",
        "inputs.requires_grad_(True)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        y = model(inputs)\n",
        "    with record_function(\"backpropagation\"):\n",
        "        out = loss(y, targets)\n",
        "        out.backward()\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inY_a9tqzrRH",
        "outputId": "f3da31ef-4a55-45d8-af3c-47b5e06ac967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        backpropagation         0.13%      20.795ms        60.12%        9.469s        9.469s             1  \n",
            "autograd::engine::evaluate_function: ConvolutionBack...         0.09%      13.563ms        52.29%        8.236s      53.135ms           155  \n",
            "                                   ConvolutionBackward0         0.02%       3.295ms        51.67%        8.139s      52.508ms           155  \n",
            "                             aten::convolution_backward        51.48%        8.108s        51.65%        8.135s      52.486ms           155  \n",
            "                                        model_inference         0.37%      58.630ms        39.88%        6.281s        6.281s             1  \n",
            "                                           aten::conv2d         0.01%       1.765ms        30.28%        4.770s      30.772ms           155  \n",
            "                                      aten::convolution         0.04%       5.852ms        30.27%        4.768s      30.761ms           155  \n",
            "                                     aten::_convolution         0.02%       3.674ms        30.24%        4.762s      30.723ms           155  \n",
            "                               aten::mkldnn_convolution        30.13%        4.746s        30.21%        4.758s      30.699ms           155  \n",
            "                                       aten::batch_norm         0.01%       1.384ms         5.92%     931.830ms       6.012ms           155  \n",
            "                           aten::_batch_norm_impl_index         0.02%       2.463ms         5.91%     930.446ms       6.003ms           155  \n",
            "                                aten::native_batch_norm         5.81%     914.746ms         5.87%     923.808ms       5.960ms           155  \n",
            "autograd::engine::evaluate_function: NativeBatchNorm...         0.13%      20.264ms         5.63%     886.578ms       5.720ms           155  \n",
            "                               NativeBatchNormBackward0         0.03%       4.085ms         5.50%     866.314ms       5.589ms           155  \n",
            "                       aten::native_batch_norm_backward         5.39%     848.475ms         5.47%     862.229ms       5.563ms           155  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 15.750s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe a large increase in execution time, where the majority of the additional time is unsurprisingly spent in the backpropagation. Though a forward pass was relatively efficient, clocking at 6.3 seconds (with some added overhead from previous experiment), the total wall clock time was almost 18.2s with 65% spent in the backward pass. Despite the increased efficiency gained from bottleneck layers, much greater speedups are required to make training feasible. This motivates the migration onto GPUs."
      ],
      "metadata": {
        "id": "jbGWB-tK3E28"
      }
    }
  ]
}